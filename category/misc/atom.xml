<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Misc | Icebergist]]></title>
  <link href="http://icebergist.com/category/misc/atom.xml" rel="self"/>
  <link href="http://icebergist.com/"/>
  <updated>2017-02-03T14:50:44+01:00</updated>
  <id>http://icebergist.com/</id>
  <author>
    <name><![CDATA[Orange Iceberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Copy and sync files from/to remote server]]></title>
    <link href="http://icebergist.com/posts/copy-sync-files-from-to-remote-server/"/>
    <updated>2016-11-07T15:55:52+01:00</updated>
    <id>http://icebergist.com/posts/copy-sync-files-from-to-remote-server</id>
    <content type="html"><![CDATA[<p>Most modern web app deployments have automated scripts that perform all tasks needed to deploy the app. They handle all the dirty details, while the developer just needs to do something simple like <code>cap deploy</code>. In other words, usually you don&rsquo;t need to access the remote servers directly.</p>

<p>However, sometimes you run into one-time tasks (or less frequent tasks) that might not have been automated. For example, dumping production data and importing on local machine, syncing uploaded files between production and staging environments, etc.</p>

<p>These often involve transferring files between your local machine and remote server (or two remote servers). There are few ways you can handle this depending on what you need to transfer between servers. We are going to cover methods using <code>wget</code>, <code>scp</code>, and <code>rsync</code>.</p>

<!--more-->


<h2>wget</h2>

<p>Simplest option is to install <code>wget</code> on destination machine. <code>wget</code> is the non-interactive network downloader and you just give it the URL of the file you want to download.</p>

<p><code>wget http://example.com/some/archive.tar.gz</code></p>

<p>That would download the file to current directory.</p>

<p>The downside is that you have to put the file somewhere accessible via web, like <code>public</code> dir in Rails apps and also you should remember to remove it once you are done with it.</p>

<p>Also this only works if you have a single file, or you are able to create a single file (most likely by <a href="/posts/create-and-extract-archives-using-tar-and-gzip/">creating an archive using tar and gzip</a>).</p>

<h2>scp</h2>

<p><code>scp</code> is a remote file copy program and the name is short for <strong>s</strong>ecure <strong>c</strong>o<strong>p</strong>y. It&rsquo;s very similar to usual <code>cp</code> command with the difference that it&rsquo;s able to copy files across different computers using SSH.</p>

<p>Simplest forms of <code>scp</code> have source and destination, both of which can be either a local file or a remote file.</p>

<p>```sh</p>

<h1>copy file from server to current dir on local machine</h1>

<p>scp myuser@example.com:/home/myuser/databasedump.sql ./</p>

<h1>copy file to remote server</h1>

<p>scp ./some/localfile.txt myuser@example.com:/home/myuser/
```</p>

<p>In a sense it works exactly like <code>cp</code>. Difference is that when specifying a remote file the format looks like you concatenated SSH user@server string and normal file path (you are saying &ldquo;as user at server get this file&rdquo;).</p>

<p>There are some additional nice things about <code>scp</code>:</p>

<ul>
<li>you can use <code>-r</code> option which will recursively copy entire directories.</li>
<li>you can specify two remote files and it will copy them between remote servers.</li>
</ul>


<h2>rsync</h2>

<p><code>scp</code> is secure version of <code>rcp</code> (remote copy) program. <code>rsync</code> is faster, flexible replacement for <code>rcp</code>. It copies files either to or from a remote host, or locally on the current host (it does not support copying files between two remote hosts).</p>

<p>There are many ways you can use <code>rsync</code>. The most usual variants are:</p>

<p>```sh</p>

<h1>copy all files recursively from one local dir to another</h1>

<p>rsync ./source_dir ./destination_dir</p>

<h1>copy a file from local dir to remote server</h1>

<p>rsync -Pavz ./archive.tar.gz myuser@example.com:/home/myuser/somedata/</p>

<h1>copy all files recursively from remote server to local dir</h1>

<p>rsync -Pavz myuser@example.com:/home/myuser/somedata ./data</p>

<p>```</p>

<p>Options that were used are not strictly necessary for doing stated tasks, but they help. They are as follows:</p>

<ul>
<li><code>P</code> &ndash; same as &mdash;partial &mdash;progress. It means it will show transfer progress and if transfer breaks keep a partial copy to possibly continue on retry. Very useful for large files.</li>
<li><code>a</code> &ndash; it is a quick way of saying you want recursion and want to preserve almost everything. This is equivalent to -rlptgoD.</li>
<li><code>v</code> &ndash; be verbose.</li>
<li><code>z</code> &ndash; compress file data during the transfer.</li>
</ul>


<p>I suggest that you consult <code>man rsync</code> for more details.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Backup and Restore a PostgreSQL Database]]></title>
    <link href="http://icebergist.com/posts/backup-and-restore-a-postresql-database/"/>
    <updated>2016-10-18T07:05:38+02:00</updated>
    <id>http://icebergist.com/posts/backup-and-restore-a-postresql-database</id>
    <content type="html"><![CDATA[<p>While working on different projects and in different environments, we often need to export a dump from one database and then import it into another. A while ago <a href="http://http://orangeiceberg.com/about/" title="About Slobodan">Slobodan</a> wrote how to <a href="http://icebergist.com/posts/import-and-export-mysql-dump/" title="Import and Export mySQL dump">export and import a mySQL dump</a>, and here is a guide how do it for PostgreSQL.</p>

<h2>Export a PostgreSQL database dump</h2>

<p>To export PostgreSQL database we will need to use the <a href="https://www.postgresql.org/docs/current/static/backup-dump.html" title="PostgreSQL">pg_dump</a> tool, which will dump all the contents of a selected database into a single file.
We need to run <code>pg_dump</code> in the command line on the computer where the database is stored. So, if the database is stored on a remote server, you will need to SSH to that server in order to run the following command:</p>

<p><code>
pg_dump -U db_user -W -F t db_name &gt; /path/to/your/file/dump_name.tar
</code>
Here we used the following options:</p>

<ul>
<li><code>-U</code> to specify which user will connect to the PostgreSQL database server.</li>
<li><code>-W</code> or <code>--password</code> will force pg_dump to prompt for a password before connecting to the server.</li>
<li><code>-F</code> is used to specify the format of the output file, which can be one of the following:

<ul>
<li><code>p</code> &ndash; plain-text SQL script</li>
<li><code>c</code> &ndash; custom-format archive</li>
<li><code>d</code> &ndash; directory-format archive</li>
<li><code>t</code> &ndash; tar-format archive</li>
</ul>
</li>
</ul>


<p><sup><em>custom</em>, <em>directory</em> and <em>tar</em> formats are suitable for input into pg_restore.</sup></p>

<p>To see a list of all the available options use <code>pg_dump -?</code>.</p>

<p>With given options <code>pg_dump</code> will first prompt for a password for the database user <code>db_user</code> and then connect as that user to the database named <code>db_name</code>. After it successfully connects, <code>&gt;</code> will write the output produced by pg_dump to a file with a given name, in this case <code>dump_name.tar</code>.</p>

<p>File created in the described process contains all the SQL queries that are required in order to replicate your database.</p>

<h2>Import a PostgreSQL database dump</h2>

<p>There are two ways to restore a PostgreSQL database:</p>

<ol>
<li><code>psql</code> for restoring from a plain SQL script file created with <code>pg_dump</code>,</li>
<li><code>pg_restore</code> for restoring from a .tar file, directory, or custom format created with <code>pg_dump</code>.</li>
</ol>


<h3>1. Restore a database with psql</h3>

<p>If your backup is a plain-text file containing SQL script, then you can restore your database by using <a href="https://www.postgresql.org/docs/current/static/app-psql.html">PostgreSQL interactive terminal</a>, and running the following command:</p>

<p><code>
psql -U db_user db_name &lt; dump_name.sql
</code>
where <code>db_user</code> is the database user, <code>db_name</code> is the database name, and <code>dump_name.sql</code> is the name of your backup file.</p>

<h3>2. Restore a database with pg_restore</h3>

<p>If you choose custom, directory, or archive format when creating a backup file, then you will need to use pg_restore in order to restore your database:</p>

<p><code>pg_restore -d db_name /path/to/your/file/dump_name.tar -c -U db_user</code></p>

<p>If you use pg_restore you have various options available, for example:</p>

<ul>
<li><code>-c</code> to drop database objects before recreating them,</li>
<li><code>-C</code> to create a database before restoring into it,</li>
<li><code>-e</code> exit if an error has encountered,</li>
<li><code>-F format</code> to specify the format of the archive.</li>
</ul>


<p>Use <code>pg_restore -?</code> to get the full list of available options.</p>

<p>You can find more info on using mentioned tools by running <code>man pg_dump</code>, <code>man psql</code> and <code>man pg_restore</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Import and export MySQL dump]]></title>
    <link href="http://icebergist.com/posts/import-and-export-mysql-dump/"/>
    <updated>2015-12-02T10:44:23+01:00</updated>
    <id>http://icebergist.com/posts/import-and-export-mysql-dump</id>
    <content type="html"><![CDATA[<p>Another simple task that&rsquo;s often hard for beginners is importing and exporting MySQL dumps. Here is quick rundown on how to do it.</p>

<p>To export data you need to use <code>mysqldump</code>:</p>

<p><code>sh
mysqldump -u db_user -p db_name &gt; dump_name.sql
</code></p>

<p>Options given to <code>mysqldump</code> are:</p>

<ul>
<li><code>-u db_user</code> &ndash; connect as user <code>db_user</code> to database</li>
<li><code>-p</code> &ndash; use password, it will ask you to enter your password</li>
<li><code>db_name</code> is the name of MySQL database you want to dump</li>
<li><code>&gt; dump_name.sql</code> &ndash; by default <code>mysqldump</code> will print out the dump to terminal, but simple output redirect with <code>&gt;</code> will instead write it to given filename, in this case <code>dump_name.sql</code></li>
</ul>


<p>Now that you have <code>dump_name.sql</code> file with all SQL queries needed to replicate your database you can import it using general-purpose <code>mysql</code> client:</p>

<p><code>sh
mysql -u db_user -p db_name &lt; dump_name.sql
</code></p>

<p>User, password, and database name options are the same as for <code>mysqldump</code>. Since <code>mysql</code> reads input from terminal this time we can use <code>&lt;</code> to read input from given file instead.</p>

<p>As always for more information you can consult manual using <code>man mysqldump</code> and <code>man mysql</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Create and extract archives using tar and gzip]]></title>
    <link href="http://icebergist.com/posts/create-and-extract-archives-using-tar-and-gzip/"/>
    <updated>2015-11-13T13:28:36+01:00</updated>
    <id>http://icebergist.com/posts/create-and-extract-archives-using-tar-and-gzip</id>
    <content type="html"><![CDATA[<p>One of the simplest tasks is creating and extracting files using <code>tar</code> and <code>gzip</code>. Yet for most new developers this is a daunting task. These days <code>tar</code> is mostly used to simply combine a few files into a single file and then <code>gzip</code> is used to compress that file.</p>

<p>Here is a quick overview how to use <code>tar</code> and <code>gzip</code> to create and compress an archive:</p>

<p>```sh</p>

<h1>archive individual files</h1>

<p>tar -cvzf myarchive.tar.gz /path/to/file1 /path/to/file2</p>

<h1>archive whole directory</h1>

<p>tar -cvzf myarchive.tar.gz /path/to/dir</p>

<h1>archive whole directory but don&rsquo;t store full path</h1>

<p>tar -cvzf myarchive.tar.gz -C /path/to/dir ./
```</p>

<p>Options give to tar are: <code>c</code> to create new archive, <code>v</code> to be verbose, <code>z</code> to compress resulting archive with <code>gzip</code>, and <code>f</code> to write the archive to specified file. After options you can list files and dirs you want to archive.</p>

<p>In all examples we provide a full path to a file or dir we want to archive. In this case <code>tar</code> will store files in the archive using the full path. This means once you extract the files you&rsquo;ll have a complete directory structure from root dir onwards.</p>

<p>The way to avoid this is either to manually <code>cd</code> to dir in which files are stored, or to tell <code>tar</code> using <code>C</code> option to change dir before archiving files.</p>

<p>Finally to extract an archive:</p>

<p><code>sh
tar -xvzf myarchive.tar.gz
</code></p>

<p>The <code>x</code> option tells <code>tar</code> to extract the archive into current directory.</p>

<p>For more information you can consult manual using <code>man tar</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upgrade Ubuntu 8.04 LTS server]]></title>
    <link href="http://icebergist.com/posts/upgrade-ubuntu-8-dot-04-lts-server/"/>
    <updated>2014-01-06T11:36:43+01:00</updated>
    <id>http://icebergist.com/posts/upgrade-ubuntu-8-dot-04-lts-server</id>
    <content type="html"><![CDATA[<p>Note to self: here is how to upgrade Ubuntu 8.04 LTS (or any other release that is no longer supported) to newer Ubuntu release.</p>

<p>When you are upgrading unsupported release of Ubuntu if you try to do the usual <code>sudo apt-get update</code> it will most likely fail because&hellip; well, it&rsquo;s unsupported. The simple fix for this is to change your <code>/etc/apt/sources.list</code> and replace repository URLs from something like <code>us.archive.ubuntu.com</code> to <code>old-releases.ubuntu.com</code>.</p>

<p>After that you should be able follow normal upgrade procedure (use sudo if you are not root):</p>

<p><code>sh
apt-get update
apt-get install update-manager-core
do-release-upgrade
</code></p>

<p>References:</p>

<ul>
<li><a href="http://rimuhosting.com/knowledgebase/linux/distros/ubuntu" title="Rimuhosting's page on upgrading Ubuntu">Rimuhosting&rsquo;s page on upgrading Ubuntu</a></li>
<li><a href="http://askubuntu.com/questions/91815/how-to-install-software-or-upgrade-from-old-unsupported-release" title="Discussion on upgrading unsupported Ubuntu release">Discussion on upgrading unsupported Ubuntu release</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
